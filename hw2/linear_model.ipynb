{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WGlcMOmcP8MO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Id2KDZynoRWn",
    "outputId": "29741d6c-e899-4b12-821a-c74f5aca0baa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L8S2uAUlj-Zt"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('training/training-data.1m', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vrQqaQ4DkeFq"
   },
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv('drive/MyDrive/nlp/training-data.1m.conll', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZSHHrFZTl8yP"
   },
   "outputs": [],
   "source": [
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm4gE_tTn3V0",
    "outputId": "33b59a82-6b6b-4700-d42d-ac872b914fb1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         The U.S. Centers for Disease Control and Preve...\n",
       "1         When Ms. Winfrey invited Suzanne Somers to sha...\n",
       "2         Elk calling -- a skill that hunters perfected ...\n",
       "3                                                  Don 't !\n",
       "4         Fish , ranked 98th in the world , fired 22 ace...\n",
       "                                ...                        \n",
       "897686    He cited \" inefficient production \" and \" cost...\n",
       "897687    Gary Jenkins , head of fixed income research a...\n",
       "897688    The Wildcats had 15 turnovers and shot 39 perc...\n",
       "897689    What high-quality research ( or logic ) suppor...\n",
       "897690    When afflicted during a game , he would make e...\n",
       "Name: text, Length: 897691, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sCj5rlz9oKfu"
   },
   "outputs": [],
   "source": [
    "RE_STRIP_SPECIAL_CHARS = r'[^a-zA-Z0-9\\s]'\n",
    "RE_WHITESPACE = r'[\\s]+'\n",
    "RE_NUMBER = r'[0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ABPXfSesn5Pt"
   },
   "outputs": [],
   "source": [
    "def extract_token(corpus):\n",
    "    print('tokenizing sentences')\n",
    "    result = []\n",
    "    for sentence in corpus:\n",
    "        s = sentence.lower()\n",
    "        s = re.sub(RE_STRIP_SPECIAL_CHARS, '', s)\n",
    "        # s = re.sub(RE_NUMBER, '<NUMBER>', s)\n",
    "        s = re.sub(RE_NUMBER, ' ', s)\n",
    "        tokenized = nltk.word_tokenize(s)\n",
    "        filtered_sentence = [w for w in tokenized if not w in stop_words]\n",
    "        result.append(filtered_sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WrFAGqVKob8S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing sentences\n"
     ]
    }
   ],
   "source": [
    "res = extract_token(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokens):\n",
    "    vocab = []\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            vocab.append(token)\n",
    "\n",
    "    word_counter = Counter(vocab)\n",
    "    avg_doc_length = len(vocab) / len(tokens)\n",
    "    vocab = set(vocab)\n",
    "    return vocab, word_counter, avg_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLnIymzqWGlb",
    "outputId": "fb8db2f8-2178-44a9-aca2-8237e3107aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is 262416, average doc length is 13.993544549293688\n"
     ]
    }
   ],
   "source": [
    "vocab, word_counter, avg_doc_length = get_vocab(res)\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size is {vocab_size}, average doc length is {avg_doc_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183020 words that appeared less than 4 times will be substituted by <UNK>\n"
     ]
    }
   ],
   "source": [
    "remove_words = []\n",
    "remove_below = 4\n",
    "\n",
    "for word, count in word_counter.items():\n",
    "    if count < remove_below:\n",
    "        remove_words.append(word)\n",
    "\n",
    "remove_words = set(remove_words)\n",
    "print(f'{len(remove_words)} words that appeared less than {remove_below} times will be substituted by <UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substituting words\n"
     ]
    }
   ],
   "source": [
    "processed_tokens = []\n",
    "print('substituting words')\n",
    "\n",
    "for sentence in res:\n",
    "    temp = []\n",
    "    for word in sentence:\n",
    "        if word in remove_words:\n",
    "            temp.append('<UNK>')\n",
    "        else:\n",
    "            temp.append(word)\n",
    "    processed_tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is 79397, new average doc length is 13.993544549293688\n"
     ]
    }
   ],
   "source": [
    "vocab, word_counter, avg_doc_length = get_vocab(processed_tokens)\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size is {vocab_size}, new average doc length is {avg_doc_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(word_counter.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9YPDU-F8WJY_"
   },
   "outputs": [],
   "source": [
    "word_to_idx = {w: idx for idx, w in enumerate(vocab)}\n",
    "idx_to_word = {idx: w for idx, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "cvrvjbVtWXfc"
   },
   "outputs": [],
   "source": [
    "def generate_skipgram(tokens, window):\n",
    "    result = []\n",
    "    print(f'generating skipgrams with window size {window}')\n",
    "    for idx, sentence in enumerate(tokens):\n",
    "        if idx % 100000 == 0 and idx > 0:\n",
    "            print(f'processed {idx} sentences')\n",
    "        for idx, token in enumerate(sentence):\n",
    "            for i in range(idx - window, idx, 1):\n",
    "                if i >= 0:\n",
    "                    skipgram = [token, sentence[i]]\n",
    "                    result.append(skipgram)\n",
    "            for j in range(idx + 1, idx + window + 1, 1):\n",
    "                if j < len(sentence):\n",
    "                    skipgram = [token, sentence[j]]\n",
    "                    result.append(skipgram)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "spoqzva1d9p6"
   },
   "outputs": [],
   "source": [
    "def skipgram_to_idx(skipgrams, idx_dict):\n",
    "    print('creating skipgram words <-> index dictionary')\n",
    "    result = []\n",
    "    for skipgram in skipgrams:\n",
    "        result.append([idx_dict[skipgram[0]], idx_dict[skipgram[1]]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Lavf840ieuu5"
   },
   "outputs": [],
   "source": [
    "def generate_batches(skipgrams, batch_size):\n",
    "    n_batches = len(skipgrams) // batch_size\n",
    "    skipgrams = skipgrams[:n_batches*batch_size]\n",
    "    for i in range(0, len(skipgrams), batch_size):\n",
    "        context = []\n",
    "        target = []\n",
    "        batch = skipgrams[i:i+batch_size]\n",
    "        for j in range(len(batch)):\n",
    "            context.append(batch[j][0])\n",
    "            target.append(batch[j][1])\n",
    "        yield context, target    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "txvSbKAveb0X",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating skipgrams with window size 2\n",
      "processed 100000 sentences\n",
      "processed 200000 sentences\n",
      "processed 300000 sentences\n",
      "processed 400000 sentences\n",
      "processed 500000 sentences\n",
      "processed 600000 sentences\n",
      "processed 700000 sentences\n",
      "processed 800000 sentences\n",
      "got 44877554 skipgrams\n",
      "creating skipgram words <-> index dictionary\n"
     ]
    }
   ],
   "source": [
    "w = 2\n",
    "skipgrams = generate_skipgram(processed_tokens, window=w)\n",
    "print(f'got {len(skipgrams)} skipgrams')\n",
    "skipgrams_idx = skipgram_to_idx(skipgrams, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, word_dist):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.word_dist = word_dist\n",
    "        \n",
    "        # (\"orange\", \"juice\", observed=1), (\"orange\", \"king\", observed=0) \n",
    "        # => \"orange\" is context word, \"juice\" & \"king\" are target words\n",
    "        self.context_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.target_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.context_embed.weight.data.uniform_(-1, 1)\n",
    "        self.target_embed.weight.data.uniform_(-1, 1)\n",
    "    \n",
    "    def get_context_row(self, word):\n",
    "        return self.context_embed(word)\n",
    "    \n",
    "    def get_target_row(self, word):\n",
    "        return self.target_embed(word)\n",
    "    \n",
    "    def get_negative_samples(self, batch_size, k):\n",
    "        negative_samples = torch.multinomial(self.word_dist, batch_size * k, replacement=True)\n",
    "        device = \"cuda\" if self.target_embed.weight.is_cuda else \"cpu\"\n",
    "        negative_samples = negative_samples.to(device)\n",
    "        return self.target_embed(negative_samples).view(batch_size, k, self.embed_dim)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, context_vectors, target_vectors, negative_vectors):\n",
    "        batch_size, embed_dim = context_vectors.shape\n",
    "        context_vectors = context_vectors.view(batch_size, embed_dim, 1)\n",
    "        target_vectors = target_vectors.view(batch_size, 1, embed_dim)\n",
    "        \n",
    "        observed_sample_loss = torch.bmm(target_vectors, context_vectors).sigmoid().log()\n",
    "        observed_sample_loss = observed_sample_loss.squeeze()\n",
    "        \n",
    "        negative_sample_loss = torch.bmm(negative_vectors.neg(), context_vectors).sigmoid().log()\n",
    "        negative_sample_loss = negative_sample_loss.squeeze().sum(1)\n",
    "        \n",
    "        return -(observed_sample_loss + negative_sample_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 0/8\n",
      "Loss:  0.8400945663452148\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 1/8\n",
      "Loss:  0.8830307126045227\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 2/8\n",
      "Loss:  0.7795015573501587\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 3/8\n",
      "Loss:  0.8539145588874817\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 4/8\n",
      "Loss:  0.8403704166412354\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 5/8\n",
      "Loss:  0.7971633076667786\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 6/8\n",
      "Loss:  0.8513500690460205\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Epoch: 7/8\n",
      "Loss:  0.855947732925415\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "word_freq = np.asarray(sorted(word_counter.values(), reverse=True))\n",
    "unigram_dist = word_freq / word_freq.sum()\n",
    "negative_sample_dist = torch.from_numpy(unigram_dist**(0.75) / np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "embed_dim = 100\n",
    "model = SkipgramModel(vocab_size, embed_dim, negative_sample_dist).to(device)\n",
    "criterion = SkipgramLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "\n",
    "print_every = 1\n",
    "epochs = 8\n",
    "k = 4\n",
    "batch_size = 512\n",
    "\n",
    "print('training started')\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    counter=0\n",
    "    \n",
    "    # get our input, target batches\n",
    "    for context_words, target_words in generate_batches(skipgrams_idx, batch_size):\n",
    "        context, targets = torch.LongTensor(context_words), torch.LongTensor(target_words)\n",
    "        context, targets = context.to(device), targets.to(device)\n",
    "\n",
    "        # input, outpt, and noise vectors\n",
    "        context_vectors = model.get_context_row(context)\n",
    "        target_vectors = model.get_target_row(targets)\n",
    "        negative_vectors = model.get_negative_samples(batch_size, k)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss = criterion(context_vectors, target_vectors, negative_vectors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        counter+=1\n",
    "        if counter % 10000 == 0:\n",
    "            print(counter)\n",
    "        \n",
    "\n",
    "    # loss stats\n",
    "    if e % print_every == 0:\n",
    "        print(f\"Epoch: {e}/{epochs}\")\n",
    "        print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_embed(embed):\n",
    "    print('writing embedding to output file')\n",
    "    f = open('embedding.txt', 'w')\n",
    "    for idx, word_embed in enumerate(embed):\n",
    "        word = idx_to_word[idx]\n",
    "        temp = word + ' '\n",
    "        for value in word_embed:\n",
    "            temp = temp + str(value) + ' '\n",
    "        temp += '\\n'\n",
    "        f.write(temp)\n",
    "    f.close()\n",
    "    print('completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing embedding to output file\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.context_embed.weight.to('cpu').data.numpy()\n",
    "output_embed(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='<UNK> 123 456'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123 456'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=embeddings[word_to_idx['boston']]\n",
    "b=embeddings[word_to_idx['irvine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.289817"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9675"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05414752, -0.0376316 , -0.02607704, -0.06123179,  0.00534598,\n",
       "        0.7662117 ,  0.03429497,  0.11548834,  0.04311212,  0.02372784,\n",
       "       -0.07481746,  0.10039123, -0.15158641, -0.0273816 ,  0.28386605,\n",
       "       -0.00294422, -0.11725447,  0.09890588,  0.06209876, -0.1405471 ,\n",
       "       -0.20329618, -0.06901078, -0.03143039,  0.11849811,  0.02379608,\n",
       "       -0.04710384,  0.09432679, -0.13477474,  0.00616562,  0.05474778,\n",
       "        0.01303839,  0.05359967, -0.06368975,  0.04925928, -0.02631722,\n",
       "        0.12471944,  0.11074349,  0.38512275,  0.07833233, -0.07567672,\n",
       "       -0.16856281,  0.01495748,  0.07998393,  0.05758069, -0.03740498,\n",
       "       -0.18101503,  0.13332672, -0.30309963,  0.0449719 , -0.12572257,\n",
       "       -0.10403565,  0.10048373, -0.04887088,  0.19241288,  0.0100302 ,\n",
       "       -0.02922419,  0.03214812, -0.11194465, -0.08714218, -0.04433919,\n",
       "        0.18598603,  0.0391029 ,  0.12263826, -0.04211675, -0.06916683,\n",
       "        0.14960961, -0.01032088,  0.00958018,  0.05241602, -0.0260222 ,\n",
       "        0.02813025,  0.091014  ,  0.05949929,  0.01529243, -0.00810464,\n",
       "        0.00551127,  0.10914782, -0.1169248 , -0.04628974, -0.08442102,\n",
       "        0.03617362, -0.4029322 ,  0.00097175, -0.12439501, -0.23014462,\n",
       "       -0.01971709,  0.00394754, -0.12626307, -0.05298414,  0.1609879 ,\n",
       "       -0.13425016,  0.23675178, -0.09495647,  0.21115924, -0.07227927,\n",
       "        0.00476232,  0.08432518, -0.06443097,  0.02843004,  0.06759436],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[9675]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_layer(word_idx):\n",
    "    input_layer = torch.zeros(vocab_size).float()\n",
    "    input_layer[word_idx] = 1.0\n",
    "    return input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing NN\n",
      "training started\n",
      "Loss at epo 0: 18.454513640520123\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e1d8b9f0b119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('initializing NN')\n",
    "embedding_dim = 50\n",
    "W2 = Variable(torch.randn(vocab_size, embedding_dim).float(), requires_grad = True)\n",
    "W1 = Variable(torch.randn(embedding_dim, vocab_size).float(), requires_grad = True)\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "print('training started')\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in skipgrams_idx:\n",
    "        x = Variable(generate_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "    if epo % 50 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(skipgrams_idx)}')\n",
    "    else:\n",
    "        print(epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp-a2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
